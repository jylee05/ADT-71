#!/usr/bin/env python3
"""
N2N-Flow2 Inference Script (Refactored)
Matches evaluate.py logic: Stitching grids first, then peak picking.
"""

import argparse
import torch
import torch.nn.functional as F
import torchaudio
import numpy as np
import pretty_midi
from scipy.signal import find_peaks
from torchaudio.transforms import MelSpectrogram, AmplitudeToDB
from tqdm import tqdm

from src.config import Config
from src.model import FlowMatchingTransformer, AnnealedPseudoHuberLoss

# Representative MIDI Notes for 7 drum classes (matching utils.py)
DRUM_MAPPING = [36, 38, 42, 47, 49, 51, 56]

class ADTInference:
    def __init__(self, args):
        
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.config = Config()
        
        print(f"ğŸ¯ N2N-Flow2 Inference (Stitch-First Mode)")
        print(f"   Device: {self.device}")
        print(f"   Sampling Steps: {args.steps}")
        print(f"   Threshold: {args.threshold}")
        
        # Load model
        self.model = FlowMatchingTransformer(self.config).to(self.device)
        self.loss_fn = AnnealedPseudoHuberLoss(self.model, self.config).to(self.device)
        self.load_checkpoint(args.ckpt_path)
        
        self.model.eval()
        self.init_feature_extractors()

    def load_checkpoint(self, path):
        print(f"ğŸ“‚ Loading checkpoint: {path}")
        checkpoint = torch.load(path, map_location=self.device, weights_only=False)
        state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
        
        if any(k.startswith('module.') for k in state_dict.keys()):
            new_state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
        else:
            new_state_dict = state_dict
            
        self.model.load_state_dict(new_state_dict)
        epoch = checkpoint.get('epoch', 'Unknown')
        print(f"   Loaded from epoch: {epoch}")

    def init_feature_extractors(self):
        self.mel_transform = MelSpectrogram(
            sample_rate=self.config.AUDIO_SR,
            n_fft=self.config.N_FFT,
            hop_length=self.config.HOP_LENGTH,
            n_mels=self.config.N_MELS,
            normalized=True
        ).to(self.device)
        
        self.db_transform = AmplitudeToDB().to(self.device)
        self.resampler_to_44k = {}
        self.resampler_to_24k = {}

    def get_features(self, waveform_segment, sr):
        # 1. Mel-Spectrogram
        if sr != self.config.AUDIO_SR:
            if sr not in self.resampler_to_44k:
                self.resampler_to_44k[sr] = torchaudio.transforms.Resample(sr, self.config.AUDIO_SR).to(self.device)
            waveform_mel = self.resampler_to_44k[sr](waveform_segment).to(self.device)
        else:
            waveform_mel = waveform_segment.to(self.device)

        melspec = self.mel_transform(waveform_mel)
        melspec = self.db_transform(melspec)
        melspec = melspec.transpose(1, 2)

        # 2. MERT Waveform
        target_mert_sr = self.config.MERT_SR
        if sr != target_mert_sr:
            if sr not in self.resampler_to_24k:
                self.resampler_to_24k[sr] = torchaudio.transforms.Resample(sr, target_mert_sr).to(self.device)
            waveform_mert = self.resampler_to_24k[sr](waveform_segment.to(self.device))
        else:
            waveform_mert = waveform_segment.to(self.device)

        waveform_mert = waveform_mert.squeeze(0).unsqueeze(0)
        return waveform_mert, melspec

    def predict_segment(self, waveform_mert, spec):
        with torch.no_grad():
            predictions = self.loss_fn.sample(
                waveform_mert, spec, 
                steps=self.args.steps
            )
            return predictions[0].cpu().numpy()

    # [ì¶”ê°€] evaluate.pyì˜ í•µì‹¬ ë¡œì§ì¸ Stitching í•¨ìˆ˜ êµ¬í˜„
    def stitch_predictions(self, segment_results, total_duration):
        """Stitch overlapping segment predictions into a single full-length grid"""
        # ì „ì²´ ê¸¸ì´ì— ë§ëŠ” ë¹ˆ Grid ìƒì„±
        grid_length = int(total_duration * self.config.FPS) + 1
        drum_channels = self.config.DRUM_CHANNELS
        
        # (Frame, Channel * 2) í¬ê¸°ì˜ ë°°ì—´: ì•ìª½ì€ Onset, ë’¤ìª½ì€ Velocity
        stitched_pred = np.full((grid_length, drum_channels * 2), -1.0, dtype=np.float32)
        overlap_count = np.zeros(grid_length, dtype=int)
        
        # ì‹œê°„ìˆœ ì •ë ¬
        segment_results.sort(key=lambda x: x['start_time'])
        
        for segment in segment_results:
            start_frame = int(segment['start_time'] * self.config.FPS)
            pred_grid = segment['pred_grid']
            
            # í˜„ì¬ Gridê°€ ì „ì²´ ê¸¸ì´ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì •
            segment_length = min(pred_grid.shape[0], grid_length - start_frame)
            
            if segment_length > 0:
                for frame_idx in range(segment_length):
                    global_frame = start_frame + frame_idx
                    
                    if overlap_count[global_frame] == 0:
                        # í•´ë‹¹ í”„ë ˆì„ì— ì²« ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ê·¸ëŒ€ë¡œ ëŒ€ì…
                        stitched_pred[global_frame] = pred_grid[frame_idx]
                    else:
                        # ì´ë¯¸ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°(Overlap êµ¬ê°„), í‰ê·  ê³„ì‚° (Running Average)
                        alpha = 1.0 / (overlap_count[global_frame] + 1)
                        stitched_pred[global_frame] = (1 - alpha) * stitched_pred[global_frame] + alpha * pred_grid[frame_idx]
                    
                    overlap_count[global_frame] += 1
                    
        return stitched_pred

    def postprocess_predictions(self, predictions, onset_threshold=0.5):
        """
        [ë³€ê²½] ì „ì²´ ê³¡ ê¸¸ì´ì˜ Gridë¥¼ ë°›ì•„ì„œ í•œ ë²ˆì— ì²˜ë¦¬í•©ë‹ˆë‹¤.
        Evaluate.pyì˜ calculate_file_metrics ë¡œì§ê³¼ ìœ ì‚¬í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.
        """
        # seq_len = predictions.shape[0]  <-- ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
        drum_channels = self.config.DRUM_CHANNELS
        
        # FIX: interleaved [on,vel,on,vel,...] -> (T, D, 2)
        pred_view = predictions.reshape(predictions.shape[0], drum_channels, 2)
        onset_pred = pred_view[:, :, 0]
        velocity_pred = pred_view[:, :, 1]

        # Velocity de-normalization: [-1, 1] -> [1, 127]
        velocity_norm = np.clip(((velocity_pred + 1) / 2) * 127, 1, 127)

        
        drum_events = []
        
        for drum_idx in range(drum_channels):
            # score(-1~1) ê¸°ì¤€ thresholdë¡œ peak picking
            peaks, _ = find_peaks(
                onset_pred[:, drum_idx],
                height=onset_threshold,
                distance=int(0.05 * self.config.FPS)
            )
            
            for peak in peaks:
                time = peak / self.config.FPS  # ì „ì²´ ê³¡ ê¸°ì¤€ì˜ ì ˆëŒ€ ì‹œê°„
                velocity = int(velocity_norm[peak, drum_idx])
                midi_note = DRUM_MAPPING[drum_idx]
                onset_score = float(onset_pred[peak, drum_idx])
                
                drum_events.append({
                    'time': time,
                    'drum': drum_idx,
                    'midi_note': midi_note,
                    'velocity': velocity,
                    'onset_score': onset_score
                })
        return sorted(drum_events, key=lambda x: x['time'])

    def create_midi_file(self, drum_events, output_path, total_duration):
        pm = pretty_midi.PrettyMIDI()
        drum_program = pretty_midi.instrument_name_to_program('Synth Drum')
        drums = pretty_midi.Instrument(program=drum_program, is_drum=True)
        
        for event in drum_events:
            note = pretty_midi.Note(
                velocity=event['velocity'],
                pitch=event['midi_note'],
                start=event['time'],
                end=event['time'] + 0.1
            )
            drums.notes.append(note)
        
        pm.instruments.append(drums)
        pm.write(output_path)
        print(f"ğŸ’¾ MIDI saved: {output_path} ({len(drum_events)} notes)")

    def process_audio_file(self, input_path, output_path):
        print(f"ğŸµ Processing: {input_path}")
        
        waveform, sr = torchaudio.load(input_path)
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        
        total_duration = waveform.shape[1] / sr
        segment_duration = self.config.SEGMENT_SEC
        segment_samples = int(segment_duration * sr)
        
        # [ë³€ê²½] Overlap ë¹„ìœ¨ì„ evaluate.pyì™€ ë™ì¼í•˜ê²Œ 0.5(50%)ë¡œ ìƒí–¥ ì¡°ì •
        # Stitching íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ ê²¹ì¹˜ëŠ” êµ¬ê°„ì„ ëŠ˜ë¦¼
        overlap_ratio = 0.5 
        hop_duration = segment_duration * (1 - overlap_ratio)
        
        num_segments = int(np.ceil((total_duration - segment_duration) / hop_duration)) + 1
        if total_duration <= segment_duration:
            num_segments = 1
        
        # [ë³€ê²½] ì´ë²¤íŠ¸ë¥¼ ë°”ë¡œ ì¶”ì¶œí•˜ì§€ ì•Šê³ , Grid(ì˜ˆì¸¡ ê²°ê³¼)ë¥¼ ëª¨ìë‹ˆë‹¤.
        segment_results = []
        
        for seg_idx in tqdm(range(num_segments), desc="Processing segments"):
            start_time = seg_idx * hop_duration
            start_sample = int(start_time * sr)
            end_sample = min(start_sample + segment_samples, waveform.shape[1])
            
            segment = waveform[:, start_sample:end_sample]
            
            if segment.shape[1] < segment_samples:
                pad_length = segment_samples - segment.shape[1]
                segment = F.pad(segment, (0, pad_length))
            
            waveform_mert, melspec = self.get_features(segment, sr)
            predictions = self.predict_segment(waveform_mert, melspec)
            
            # [ë³€ê²½] ì˜ˆì¸¡ëœ Gridì™€ ì‹œì‘ ì‹œê°„ì„ ì €ì¥
            segment_results.append({
                'start_time': start_time,
                'pred_grid': predictions
            })
            
        # [ì¶”ê°€] ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ê°€ ëë‚œ í›„ Stitching ìˆ˜í–‰
        print("ğŸ”„ Stitching segments...")
        stitched_grid = self.stitch_predictions(segment_results, total_duration)
        
        # [ì¶”ê°€] í•©ì³ì§„ ì „ì²´ Gridì—ì„œ ë…¸íŠ¸ ì¶”ì¶œ (Event Detection)
        print("ğŸ¹ extracting notes...")
        all_events = self.postprocess_predictions(stitched_grid, self.args.threshold)
        
        # MIDI ìƒì„±
        self.create_midi_file(all_events, output_path, total_duration)
        
        return all_events

def parse_args():
    parser = argparse.ArgumentParser(description='N2N-Flow2 Inference (Stitch-First)')
    parser.add_argument('--ckpt_path', type=str, required=True, help='Path to model checkpoint')
    parser.add_argument('--input', type=str, required=True, help='Input audio file or directory')
    parser.add_argument('--output', type=str, required=True, help='Output directory for MIDI files')
    parser.add_argument('--steps', type=int, default=5, help='Number of sampling steps')
    parser.add_argument('--threshold', type=float, default=0.0, help='Onset score threshold (score in [-1, 1]; default 0.0)')
    return parser.parse_args()

def main():
    args = parse_args()
    os.makedirs(args.output, exist_ok=True)
    inferencer = ADTInference(args)
    
    if os.path.isfile(args.input):
        input_name = os.path.splitext(os.path.basename(args.input))[0]
        output_path = os.path.join(args.output, f"{input_name}_transcription.mid")
        inferencer.process_audio_file(args.input, output_path)
        
    elif os.path.isdir(args.input):
        import glob
        audio_extensions = ['.wav', '.mp3', '.flac', '.m4a']
        audio_files = []
        for ext in audio_extensions:
            audio_files.extend(glob.glob(os.path.join(args.input, f"**/*{ext}"), recursive=True))
        
        print(f"ğŸ“ Found {len(audio_files)} audio files")
        
        for audio_file in tqdm(audio_files, desc="Processing files"):
            input_name = os.path.splitext(os.path.basename(audio_file))[0]
            output_path = os.path.join(args.output, f"{input_name}_transcription.mid")
            try:
                inferencer.process_audio_file(audio_file, output_path)
            except Exception as e:
                print(f"âŒ Error processing {audio_file}: {e}")
    else:
        print(f"âŒ Invalid input path: {args.input}")

if __name__ == "__main__":
    main()
